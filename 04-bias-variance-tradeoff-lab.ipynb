{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/proffranciscofernando/introduction-to-data-science/blob/main/04-bias-variance-tradeoff-lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Bias and Variance Tradeoff"
      ],
      "metadata": {
        "id": "_t7lBzQbcyBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Introduction\n",
        "In this notebook, we will explore the concepts of bias and variance, which are crucial in understanding the performance of machine learning models. We will demonstrate these concepts using synthetic data, simple linear regression models, and cross-validation."
      ],
      "metadata": {
        "id": "TQppiCV1c71u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Importing Libraries"
      ],
      "metadata": {
        "id": "OO7UKgRNdF6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.evaluate import bias_variance_decomp"
      ],
      "metadata": {
        "id": "XZNpjR5bc-te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creating Synthetic Data"
      ],
      "metadata": {
        "id": "QPBGMbffdUH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate synthetic data\n",
        "def generate_synthetic_data(n_samples=100, noise=1.0, random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    X = np.linspace(0, 100, n_samples).reshape(-1, 1)\n",
        "    true_function = -0.0001 * X**3 + 0.01 * X**2 + 0.1 * X + 1\n",
        "    y = true_function + np.random.normal(scale=noise, size=X.shape)\n",
        "    return X, y, true_function\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y, true_function = generate_synthetic_data(n_samples=100, noise=2.0)"
      ],
      "metadata": {
        "id": "JUGb7mHqdYfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sbGBTOhOddBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Simple Linear Regression Model"
      ],
      "metadata": {
        "id": "ri_ha2VfdfjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a simple linear regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = lin_reg.predict(X_train)\n",
        "y_test_pred = lin_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "rBAb5v7AdkWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
        "plt.scatter(X_test, y_test, color='green', label='Testing data')\n",
        "plt.plot(X_train, y_train_pred, color='red', label='Linear regression')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.title('Linear Regression Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sGXxKx6qdn93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating training and testing errors\n",
        "train_error = mean_squared_error(y_train, y_train_pred)\n",
        "test_error = mean_squared_error(y_test, y_test_pred)\n",
        "print(f'Training Error: {train_error}')\n",
        "print(f'Testing Error: {test_error}')"
      ],
      "metadata": {
        "id": "lddigFYNdqIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, why training MSE is greater than test MSE?"
      ],
      "metadata": {
        "id": "Q6E7LgjaF9Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Polynomial Regression Models"
      ],
      "metadata": {
        "id": "HfvM3QzPdtrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and plot polynomial models of different degrees\n",
        "degrees = [1, 2, 3, 15]\n",
        "colors = ['orange', 'red', 'black', 'cyan']\n",
        "predictions = []\n",
        "\n",
        "for degree, color in zip(degrees, colors):\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_poly = poly_features.fit_transform(X)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    y_poly_pred = model.predict(X_poly)\n",
        "    predictions.append(y_poly_pred)\n",
        "    plt.plot(X, y_poly_pred, label=f'Degree {degree}', color=color)\n",
        "\n",
        "# Plot the synthetic data and the true function\n",
        "plt.scatter(X, y, facecolors='none', edgecolors='black', label='Data')\n",
        "plt.plot(X, true_function, label='True Function', color='blue')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (Y)')\n",
        "plt.title('Data and Polynomial Fits')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FhBC9VROG4SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Bias-Variance Tradeoff"
      ],
      "metadata": {
        "id": "rfaVy7XOd2sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias-variance tradeoff is a fundamental concept in machine learning. It describes the tradeoff between two sources of error that affect the performance of a model:\n",
        "\n",
        "- **Bias**: Error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss the relevant relations between features and target outputs (underfitting).\n",
        "- **Variance**: Error due to too much complexity in the learning algorithm. High variance can cause the model to model the random noise in the training data (overfitting).\n",
        "\n",
        "To visualize the bias-variance tradeoff, we can plot the training and testing errors for models of varying complexity."
      ],
      "metadata": {
        "id": "InhiiNRUd_0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot training and test MSE\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "flexibility = []\n",
        "\n",
        "for degree in range(1, 21):\n",
        "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    X_test_poly = poly_features.fit_transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train_poly)\n",
        "    y_test_pred = model.predict(X_test_poly)\n",
        "\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
        "    flexibility.append(degree)\n",
        "\n",
        "plt.plot(flexibility, train_errors, label='Training MSE', color='gray', marker='o')\n",
        "plt.plot(flexibility, test_errors, label='Test MSE', color='red', marker='o')\n",
        "plt.xlabel('Flexibility (Polynomial Degree)')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Bias-Variance Tradeoff')\n",
        "plt.xticks(flexibility)  # Show integer values of degrees on the x-axis\n",
        "plt.legend()\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DK99WHgxFXHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion"
      ],
      "metadata": {
        "id": "gdMFSEy0eE7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explored the concepts of bias and variance and demonstrated how they affect the performance of machine learning models. We trained linear and polynomial regression models on synthetic data and visualized the bias-variance tradeoff.\n",
        "\n",
        "Understanding and managing the bias-variance tradeoff is crucial for building models that generalize well to new, unseen data. By balancing bias and variance, we can achieve better performance and more reliable predictions."
      ],
      "metadata": {
        "id": "YOaQ_76pelgR"
      }
    }
  ]
}